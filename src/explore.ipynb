{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Natural Language Processing (NLP)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Table of Contents\n",
                "\n",
                "* [0. Problem Statement](#co)\n",
                "* [1. Importing libraries](#c1)\n",
                "* [2. Data Collection](#c2)\n",
                "* [3. Exploration and Data Cleaning](#c3)\n",
                "  * [3.1 Drop Null Values](#c3-1)\n",
                "  * [3.2 Drop Duplicate Information](#c3-2)\n",
                "* [4. Preprocessing of Text (URLs)](#c4)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Problem Statement <a id='c0'></a>\n",
                "\n",
                "The objective of this exercise is to develop an NLP model to detect spam on a webpage based on its URL."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Importing libraries <a id='c1'></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "import os\n",
                "import json\n",
                "import warnings\n",
                "import pickle\n",
                "from pickle import dump\n",
                "import regex as re\n",
                "from nltk import download\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "def warn(*args, **kwargs):\n",
                "    pass\n",
                "warnings.warn = warn\n",
                "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
                "pd.set_option('display.max_columns', None)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Collection <a id='c2'></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                                                 url  is_spam\n",
                        "0  https://briefingday.us8.list-manage.com/unsubs...     True\n",
                        "1                             https://www.hvper.com/     True\n",
                        "2                 https://briefingday.com/m/v4n3i4f3     True\n",
                        "3   https://briefingday.com/n/20200618/m#commentform    False\n",
                        "4                        https://briefingday.com/fan     True\n"
                    ]
                }
            ],
            "source": [
                "URL = 'https://breathecode.herokuapp.com/asset/internal-link?id=932&path=url_spam.csv'\n",
                "\n",
                "def get_data(URL:str) -> pd.DataFrame:\n",
                "    total_data = pd.read_csv(URL, sep=',')\n",
                "    total_data.head()\n",
                "    return total_data\n",
                "\n",
                "get_data(URL)\n",
                "total_data = get_data(URL)\n",
                "print(total_data.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Exploration and Data Cleaning <a id='c3'></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 3.1 Drop Null Values <a id='c3-1'></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Since there is no null values, we won't delete any data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "url        0\n",
                            "is_spam    0\n",
                            "dtype: int64"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "total_data.isna().sum()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 3.2 Drop Duplicate Information <a id='c3-2'></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Before: The dataframe has 2999 rows, of which 630 are duplicated.\n",
                        "After: Now the dataframe has 2369 rows and 0 duplicated ones.\n"
                    ]
                }
            ],
            "source": [
                "rows_before = total_data.shape[0]\n",
                "duplicated_rows_before = total_data.duplicated().sum()\n",
                "print(f'Before: The dataframe has {rows_before} rows, of which {duplicated_rows_before} are duplicated.')\n",
                "\n",
                "total_data = total_data.drop_duplicates()\n",
                "\n",
                "rows_after = total_data.shape[0]\n",
                "duplicated_rows_after = total_data.duplicated().sum()\n",
                "total_data = total_data.reset_index(inplace=False, drop=True)\n",
                "print(f'After: Now the dataframe has {rows_after} rows and {duplicated_rows_after} duplicated ones.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Preprocessing of Text (URLs) <a id='c4'></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.1 Encoding Categorical Values and Saving JSON Files <a id='c4-1'></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "spam_legend = {\n",
                "    '0': 'Not Spam',\n",
                "    '1': 'Spam'\n",
                "}\n",
                "\n",
                "total_data['is_spam'] = total_data['is_spam'].astype(int)\n",
                "total_data.head()\n",
                "\n",
                "output_dir = 'factorized_data'\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "filepath = os.path.join(output_dir, 'is_spam.json')\n",
                "\n",
                "with open(filepath, 'w') as f:\n",
                "    json.dump(spam_legend, f, indent=4)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2  <a id='c4-2'></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n",
                        "[nltk_data]   Package wordnet is already up-to-date!\n",
                        "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
                        "[nltk_data]   Package stopwords is already up-to-date!\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "lemmatize_text() missing 1 required positional argument: 'words'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     tokens = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) > \u001b[32m3\u001b[39m]\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m total_data[\u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m] = total_data[\u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m].apply(\u001b[43mlemmatize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     14\u001b[39m total_data.head()\n",
                        "\u001b[31mTypeError\u001b[39m: lemmatize_text() missing 1 required positional argument: 'words'"
                    ]
                }
            ],
            "source": [
                "download(\"wordnet\")\n",
                "download(\"stopwords\")\n",
                "\n",
                "lemmatizer = WordNetLemmatizer\n",
                "stop_words = stopwords.words('english')\n",
                "\n",
                "def lemmatize_text(words, lemmatizer = lemmatizer):\n",
                "    tokens = [lemmatizer.lemmatize(word) for word in words]\n",
                "    tokens = [word for word in tokens if word not in stop_words]\n",
                "    tokens = [word for word in tokens if len(word) > 3]\n",
                "    return tokens\n",
                "\n",
                "total_data[\"url\"] = total_data[\"url\"].apply(lemmatize_text())\n",
                "total_data.head()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
