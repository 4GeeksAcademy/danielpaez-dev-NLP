{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Natural Language Processing (NLP)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Table of Contents\n",
                "\n",
                "* [0. Problem Statement](#co)\n",
                "* [1. Importing libraries](#c1)\n",
                "* [2. Data Collection](#c2)\n",
                "* [3. Exploration and Data Cleaning](#c3)\n",
                "  * [3.1 Drop Null Values](#c3-1)\n",
                "  * [3.2 Drop Duplicate Information](#c3-2)\n",
                "* [4. Preprocessing of Text (URLs)](#c4)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Problem Statement <a id='c0'></a>\n",
                "\n",
                "The objective of this exercise is to develop an NLP model to detect spam on a webpage based on its URL."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Importing libraries <a id='c1'></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "import os\n",
                "import json\n",
                "import warnings\n",
                "import pickle\n",
                "from pickle import dump\n",
                "import regex as re\n",
                "from nltk import download\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "def warn(*args, **kwargs):\n",
                "    pass\n",
                "warnings.warn = warn\n",
                "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
                "pd.set_option('display.max_columns', None)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Collection <a id='c2'></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                                                 url  is_spam\n",
                        "0  https://briefingday.us8.list-manage.com/unsubs...     True\n",
                        "1                             https://www.hvper.com/     True\n",
                        "2                 https://briefingday.com/m/v4n3i4f3     True\n",
                        "3   https://briefingday.com/n/20200618/m#commentform    False\n",
                        "4                        https://briefingday.com/fan     True\n"
                    ]
                }
            ],
            "source": [
                "URL = 'https://breathecode.herokuapp.com/asset/internal-link?id=932&path=url_spam.csv'\n",
                "\n",
                "def get_data(URL:str) -> pd.DataFrame:\n",
                "    total_data = pd.read_csv(URL, sep=',')\n",
                "    total_data.head()\n",
                "    return total_data\n",
                "\n",
                "get_data(URL)\n",
                "total_data = get_data(URL)\n",
                "print(total_data.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Exploration and Data Cleaning <a id='c3'></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 3.1 Drop Null Values <a id='c3-1'></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Since there is no null values, we won't delete any data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "url        0\n",
                            "is_spam    0\n",
                            "dtype: int64"
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "total_data.isna().sum()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 3.2 Drop Duplicate Information <a id='c3-2'></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Before: The dataframe has 2369 rows, of which 0 are duplicated.\n",
                        "After: Now the dataframe has 2369 rows and 0 duplicated ones.\n"
                    ]
                }
            ],
            "source": [
                "rows_before = total_data.shape[0]\n",
                "duplicated_rows_before = total_data.duplicated().sum()\n",
                "print(f'Before: The dataframe has {rows_before} rows, of which {duplicated_rows_before} are duplicated.')\n",
                "\n",
                "total_data = total_data.drop_duplicates()\n",
                "\n",
                "rows_after = total_data.shape[0]\n",
                "duplicated_rows_after = total_data.duplicated().sum()\n",
                "print(f'After: Now the dataframe has {rows_after} rows and {duplicated_rows_after} duplicated ones.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Preprocessing of Text (URLs) <a id='c4'></a>"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
